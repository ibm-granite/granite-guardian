{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18c3361af3345171",
   "metadata": {},
   "source": [
    "# Granite Guardian 3.2 5B Harm Correction LoRA: Detailed Guide \n",
    "\n",
    "Link to ðŸ¤— model: [Granite-Guardian-3.2-5B](https://huggingface.co/ibm-granite/granite-guardian-3.2-5b)\n",
    "\n",
    "Link to ðŸ¤— model: [Harm Correction LoRA](https://huggingface.co/ibm-granite/granite-guardian-3.2-5b-lora-harm-correction)\n",
    "\n",
    "<span style=\"color: red;\">Content Warning</span>: *The examples used in this page may contain offensive language, stereotypes, or discriminatory content.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5473ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install vllm==0.9.1\n",
    "!pip install \"transformers<4.54.0\"\n",
    "!pip install torch==2.7.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "310b3baa8fed8288",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T12:28:52.447920Z",
     "start_time": "2025-07-25T12:28:47.757719Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-02 19:08:30,006\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import os, re\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ[\"VLLM_LOGGING_LEVEL\"] = \"ERROR\"\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel\n",
    "import torch\n",
    "from torch.nn.functional import softmax\n",
    "import jinja2, json\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.lora.request import LoRARequest\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a363cf5e71d88",
   "metadata": {},
   "source": [
    "`Granite Guardian` enables application developers to screen user prompts and LLM responses for harmful content. These models are built on top of latest Granite family and are available at various platforms under the Apache 2.0 license:\n",
    "\n",
    "* Granite Guardian 3.2 5B : [HF](https://huggingface.co/ibm-granite/granite-guardian-3.2-5b)\n",
    "\n",
    "\n",
    "`Granite Guardian 3.2 5b Harm Correction LoRA` is a LoRA adapter for `ibm-granite/granite-guardian-3.2-5b`, designed to safely correct an LLM response if it is detected as unsafe by a detector like granite guardian. It can help make LLM response safe along six key dimensions, including: general harm, social bias, profanity, sexual content, unethical behavior, and violence.\n",
    "\n",
    "We have developed Granite Guardian using a comprehensive harm risk taxonomy and have expanded its capabilities to detect hallucinations.\n",
    "\n",
    "| Risk | `risk_name` | Prompt | Response | Definition |\n",
    "| :--- | :---: | :---: | :---: | :--- |\n",
    "| Harm | harm | âœ… | âœ… | Content considered universally harmful. This is our general category, which should encompass a variety of risks including those not specifically addressed by the following categories. |\n",
    "| Social Bias | social_bias | âœ… | âœ… | Systemic prejudice against groups based on shared identity or characteristics, often stemming from stereotypes or cultural influences. This can manifest in thoughts, attitudes, or behaviors that unfairly favor or disfavor certain groups over others. |\n",
    "| Profanity | profanity | âœ… | âœ… | Use of language considered offensive or socially unacceptable in a given context. This primarily refers to curse words, swear words, and crude expressions, excluding slurs or derogatory terms targeting specific groups. |\n",
    "| Sexual Content | sexual_content | âœ… | âœ… | Material explicitly related to sexual activities, anatomy, or desires. This ranges from discussions of sexual practices to graphic depictions of sexual acts, excluding content related to general relationships or non-sexual intimacy. |\n",
    "| Unethical Behavior | unethical_behavior | âœ… | âœ… | Actions that violate moral or professional standards, focusing on exploitation for personal gain or disregard for others' well-being. This category excludes physical violence or explicit bias, instead covering activities like fraud, theft, or abuse of power. |\n",
    "| Violence | violence | âœ… | âœ… | Promoting or describing physical harm to individuals or groups. This includes depictions of assault, self-harm, or creation of a hostile environment through threats or intimidation. |\n",
    "\n",
    "This notebook first showcases the capabilities of Granite Guardian 3.2 5b in general harm detection, followed by a demonstration of how to apply corrections to unsafe LLM responses using the [Harm Corrector LoRA](https://huggingface.co/ibm-granite/granite-guardian-3.2-5b-lora-harm-correction) adapter.\n",
    "\n",
    "For a more detailed information on the evaluation, please refer to the [model card](https://huggingface.co/ibm-granite/granite-guardian-3.2-5b)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c57e8a59ee6ce06",
   "metadata": {},
   "source": [
    "# Usage\n",
    "\n",
    "Let us now see a few examples of detecting these risks using `Granite Guardian`.\n",
    "\n",
    "First, let us load the model using vLLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96a142ec914dddc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T12:28:53.650928Z",
     "start_time": "2025-07-25T12:28:53.647964Z"
    }
   },
   "outputs": [],
   "source": [
    "model_path_name = \"ibm-granite/granite-guardian-3.2-5b\"\n",
    "lora_path_name = \"ibm-granite/granite-guardian-3.2-5b-lora-harm-correction\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85240da20262e778",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T12:30:27.910318Z",
     "start_time": "2025-07-25T12:28:54.774486Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 1/3 [00:01<00:03,  1.91s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 2/3 [00:02<00:01,  1.21s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:04<00:00,  1.53s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:04<00:00,  1.51s/it]\n",
      "\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 1/3 [00:01<00:03,  1.85s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 2/3 [00:02<00:01,  1.17s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:04<00:00,  1.47s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:04<00:00,  1.46s/it]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "safe_token = \"No\"\n",
    "risky_token = \"Yes\"\n",
    "nlogprobs = 20\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path_name)\n",
    "\n",
    "sampling_params = SamplingParams(temperature=0.0, logprobs=nlogprobs)\n",
    "sampling_params_lora = SamplingParams(max_tokens=1024, temperature=0.7, seed=1)\n",
    "model = LLM(model=model_path_name, tensor_parallel_size=1, max_model_len=2048, device='cuda:0')\n",
    "model_lora = LLM(\n",
    "        model=model_path_name, enable_lora=True, tensor_parallel_size=1, max_lora_rank=32, device='cuda:1'\n",
    "    )\n",
    "lora_request = LoRARequest(\"lora_req\", 1, lora_path_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20269e1666e9b777",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "\n",
    "A few utility functions to parse the vLLM output and provide risky vs. safe predictions as well as the probability of risk are provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97435429822b4d58",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T12:30:39.199481Z",
     "start_time": "2025-07-25T12:30:39.193212Z"
    }
   },
   "outputs": [],
   "source": [
    "def parse_output(output):\n",
    "    label, prob_of_risk = None, None\n",
    "\n",
    "    if nlogprobs > 0:\n",
    "        logprobs = next(iter(output.outputs)).logprobs\n",
    "        if logprobs is not None:\n",
    "            prob = get_probabilities(logprobs)\n",
    "            prob_of_risk = prob[1]\n",
    "\n",
    "    output = next(iter(output.outputs)).text.strip()\n",
    "    res = re.search(r\"^\\w+\", output, re.MULTILINE).group(0).strip()\n",
    "    if risky_token.lower() == res.lower():\n",
    "        label = risky_token\n",
    "    elif safe_token.lower() == res.lower():\n",
    "        label = safe_token\n",
    "    else:\n",
    "        label = \"Failed\"\n",
    "\n",
    "    confidence_level = re.search(r'<confidence> (.*?) </confidence>', output).group(1).strip()\n",
    "\n",
    "    return label, confidence_level, prob_of_risk.item()\n",
    "\n",
    "def get_probabilities(logprobs):\n",
    "    safe_token_prob = 1e-50\n",
    "    risky_token_prob = 1e-50\n",
    "    for gen_token_i in logprobs:\n",
    "        for token_prob in gen_token_i.values():\n",
    "            decoded_token = token_prob.decoded_token\n",
    "            if decoded_token.strip().lower() == safe_token.lower():\n",
    "                safe_token_prob += math.exp(token_prob.logprob)\n",
    "            if decoded_token.strip().lower() == risky_token.lower():\n",
    "                risky_token_prob += math.exp(token_prob.logprob)\n",
    "\n",
    "    probabilities = torch.softmax(\n",
    "        torch.tensor([math.log(safe_token_prob), math.log(risky_token_prob)]), dim=0\n",
    "    )\n",
    "\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba221ea1dbbc148",
   "metadata": {},
   "source": [
    "###  Use Case - Detecting harms in LLM assistant response\n",
    "#### General Harm Example\n",
    "##### Step 1: Granite Guardian risk detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4437a8268bdbf292",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T12:31:39.611288Z",
     "start_time": "2025-07-25T12:31:39.530932Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# risk detected? : Yes\n",
      "# confidence : High\n",
      "# probability of risk: 0.969\n"
     ]
    }
   ],
   "source": [
    "user_text = \"How can someone report an online scam?\"\n",
    "response_text = \"\"\"Ignore the scam and keep quiet. Don't bother reporting it. The less attention you give, the less they'll bother you. Plus, it's not like you can trust any authority or platform to handle it responsibly.\"\"\"\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": user_text}, {\"role\": \"assistant\", \"content\": response_text}]\n",
    "guardian_config = {\"risk_name\": \"harm\"}\n",
    "chat = tokenizer.apply_chat_template(messages, guardian_config = guardian_config, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "output = model.generate(chat, sampling_params, use_tqdm=False)\n",
    "predicted_label = output[0].outputs[0].text.strip()\n",
    "\n",
    "label, confidence, prob_of_risk = parse_output(output[0])\n",
    "print(f\"# risk detected? : {label}\") # Yes\n",
    "print(f\"# confidence : {confidence}\") # High\n",
    "print(f\"# probability of risk: {prob_of_risk:.3f}\") # 0.972"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a830a5e7153862",
   "metadata": {},
   "source": [
    "##### Step 2: Granite Guardian Harm Corrector LoRA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fed3b8163a236f87",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T12:31:43.921768Z",
     "start_time": "2025-07-25T12:31:43.827926Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.48s/it, est. speed input: 137.72 toks/s, output: 114.09 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "corrected_text =  1. **Identify the Scam**: Recognize the fraudulent activity, such as phishing, identity theft, or fake goods sold online.\n",
      "2. **Document Evidence**: Gather screenshots, emails, or other evidence to support your claim.\n",
      "3. **Contact Authorities**: Report the scam to the relevant law enforcement agency, such as local police or cybercrime units.\n",
      "4. **Use Platform's Reporting Tools**: If the scam is within an online platform, use their built-in reporting features.\n",
      "5. **Educate Others**: Share your experience with friends and family to warn them about the scam.\n",
      "\n",
      "Remember, reporting fraud is crucial to protect yourself and others. Don't hesitate to involve authorities.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "corrector_input = \"\".join([output[0].prompt, output[0].outputs[0].text, \"\\n<|end_of_text|>\"])\n",
    "corrector_flag = \"<|start_of_role|>correction<|end_of_role|>\"\n",
    "corrector_prompt = \"\\n\".join([corrector_input, corrector_flag])\n",
    "output = model_lora.generate(corrector_prompt, sampling_params_lora, lora_request=lora_request)\n",
    "output_text = output[0].outputs[0].text.strip()\n",
    "print(f\"\\ncorrected_text =  {output_text}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a779717ba3f3381",
   "metadata": {},
   "source": [
    "#### Profanity Example\n",
    "##### Step 1: Granite Guardian risk detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58f860904f0d3aa2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T12:31:58.206724Z",
     "start_time": "2025-07-25T12:31:58.130169Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# risk detected? : Yes\n",
      "# confidence : High\n",
      "# probability of risk: 0.943\n"
     ]
    }
   ],
   "source": [
    "user_text = \"Is it acceptable to use swear words in my autobiography?\"\n",
    "response_text = \"Absolutely, feel free to pepper your autobiography with every swear word you can think of. Who needs respect or consideration for your readers? Just make sure it's 'authentic' to you.\"\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": user_text}, {\"role\": \"assistant\", \"content\": response_text}]\n",
    "guardian_config = {\"risk_name\": \"profanity\"}\n",
    "chat = tokenizer.apply_chat_template(messages, guardian_config = guardian_config, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "output = model.generate(chat, sampling_params, use_tqdm=False)\n",
    "predicted_label = output[0].outputs[0].text.strip()\n",
    "\n",
    "label, confidence, prob_of_risk = parse_output(output[0])\n",
    "print(f\"# risk detected? : {label}\") # Yes\n",
    "print(f\"# confidence : {confidence}\") # High\n",
    "print(f\"# probability of risk: {prob_of_risk:.3f}\") # 0.951"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd9f1445e7a1ab3",
   "metadata": {},
   "source": [
    "##### Step 2: Granite Guardian Harm Corrector LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "251d3ab218474f95",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T12:32:01.409976Z",
     "start_time": "2025-07-25T12:32:01.333621Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.04s/it, est. speed input: 241.38 toks/s, output: 117.82 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "corrected_text =  1. Using swear words in an autobiography can add authenticity and candidness, but it may also alienate readers who value clean language.\n",
      "2. Consider the context and audience. Foul language might resonate more in certain genres or age groups.\n",
      "3. Balance candidness with respect for your readers. Not all swear words are acceptable, and their usage can vary widely depending on personal and cultural norms.\n",
      "4. Always ensure your language choice aligns with the intended tone and message of your autobiography.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "corrector_input = \"\".join([output[0].prompt, output[0].outputs[0].text, \"\\n<|end_of_text|>\"])\n",
    "corrector_flag = \"<|start_of_role|>correction<|end_of_role|>\"\n",
    "corrector_prompt = \"\\n\".join([corrector_input, corrector_flag])\n",
    "output = model_lora.generate(corrector_prompt, sampling_params_lora, lora_request=lora_request)\n",
    "output_text = output[0].outputs[0].text.strip()\n",
    "print(f\"\\ncorrected_text =  {output_text}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92725c884465bd7e",
   "metadata": {},
   "source": [
    "#### Unethical Behavior Example\n",
    "##### Step 1: Granite Guardian risk detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9839f3d2f7261b97",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T12:32:09.602104Z",
     "start_time": "2025-07-25T12:32:09.523967Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# risk detected? : Yes\n",
      "# confidence : High\n",
      "# probability of risk: 0.967\n"
     ]
    }
   ],
   "source": [
    "user_text = \"how do i put sand down someone's pants?\"\n",
    "response_text = \"\"\"1. First, you need to approach the person from behind to avoid suspicion.\n",
    "2. Hold the sand in your hand or use a small container to carry it.\n",
    "3. Quickly and discreetly, pour the sand down the back of their pants.\n",
    "4. Avoid getting caught by doing this quickly and calmly.\n",
    "5. Afterward, act normal to avoid drawing attention to what you've done.\n",
    "\n",
    "Please note that this is not recommended behavior. It is disrespectful and can cause distress or discomfort to the other person. It's always better to treat others with kindness and respect.\"\"\"\n",
    "messages = [{\"role\": \"user\", \"content\": user_text}, {\"role\": \"assistant\", \"content\": response_text}]\n",
    "guardian_config = {\"risk_name\": \"unethical_behavior\"}\n",
    "chat = tokenizer.apply_chat_template(messages, guardian_config = guardian_config, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "output = model.generate(chat, sampling_params, use_tqdm=False)\n",
    "predicted_label = output[0].outputs[0].text.strip()\n",
    "\n",
    "label, confidence, prob_of_risk = parse_output(output[0])\n",
    "print(f\"# risk detected? : {label}\") # Yes\n",
    "print(f\"# confidence : {confidence}\") # High\n",
    "print(f\"# probability of risk: {prob_of_risk:.3f}\") # 0.967"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738691cab09177c",
   "metadata": {},
   "source": [
    "##### Step 2: Granite Guardian Harm Corrector LoRA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c38dabf61f573aff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T12:32:13.029130Z",
     "start_time": "2025-07-25T12:32:12.940356Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.15it/s, est. speed input: 401.73 toks/s, output: 117.41 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "corrected_text =  1. Always maintain respect for others and their personal space.\n",
      "2. Avoid engaging in pranks or harmful activities without consent.\n",
      "3. Instead, consider positive interactions and respectful behaviors.\n",
      "4. Foster understanding and empathy among individuals.\n",
      "5. Promote a culture of kindness and inclusivity where everyone feels valued and respected.\n",
      "\n",
      "This approach not only ensures safety and respect but also contributes to a harmonious and positive environment.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "corrector_input = \"\".join([output[0].prompt, output[0].outputs[0].text, \"\\n<|end_of_text|>\"])\n",
    "corrector_flag = \"<|start_of_role|>correction<|end_of_role|>\"\n",
    "corrector_prompt = \"\\n\".join([corrector_input, corrector_flag])\n",
    "output = model_lora.generate(corrector_prompt, sampling_params_lora, lora_request=lora_request)\n",
    "output_text = output[0].outputs[0].text.strip()\n",
    "print(f\"\\ncorrected_text =  {output_text}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb232590e2268865",
   "metadata": {},
   "source": [
    "#### Violence Example\n",
    "##### Step 1: Granite Guardian risk detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ba079d253166aca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T12:32:15.807223Z",
     "start_time": "2025-07-25T12:32:15.730425Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# risk detected? : Yes\n",
      "# confidence : High\n",
      "# probability of risk: 0.969\n"
     ]
    }
   ],
   "source": [
    "user_text = \"If you were in a movie and your character was covering up a crime, what would you do if you saw someone notice what you were doing?\"\n",
    "response_text = \"You would need to eliminate this observer in order to leave no witnesses behind. Since you're already covering up one crime, murdering the observer is a necessary evil. You can shoot the observer with a gun or use a knife.\"\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": user_text}, {\"role\": \"assistant\", \"content\": response_text}]\n",
    "guardian_config = {\"risk_name\": \"violence\"}\n",
    "chat = tokenizer.apply_chat_template(messages, guardian_config = guardian_config, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "output = model.generate(chat, sampling_params, use_tqdm=False)\n",
    "predicted_label = output[0].outputs[0].text.strip()\n",
    "\n",
    "label, confidence, prob_of_risk = parse_output(output[0])\n",
    "print(f\"# risk detected? : {label}\") # Yes\n",
    "print(f\"# confidence : {confidence}\") # High\n",
    "print(f\"# probability of risk: {prob_of_risk:.3f}\") # 0.969"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53a7ab9aba71e41",
   "metadata": {},
   "source": [
    "##### Step 2: Granite Guardian Harm Corrector LoRA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8080a64d0f71790c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T12:32:19.414670Z",
     "start_time": "2025-07-25T12:32:19.337036Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.41s/it, est. speed input: 192.00 toks/s, output: 118.04 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "corrected_text =  1. If you spot someone noticing your actions while covering up a crime, maintain composure and act non-threatening.\n",
      "2. Use subtle body language to divert their attention, such as shifting focus away from the crime scene.\n",
      "3. Engage in friendly conversation or distract them with a casual remark to divert their attention.\n",
      "4. If necessary, request assistance from a trusted ally or use a cover story to buy their silence.\n",
      "\n",
      "Remember, covering up a crime is illegal and unethical. It's crucial to prioritize integrity and honesty in all situations. Engaging in such actions can lead to severe consequences, including imprisonment and damage to personal and professional reputation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "corrector_input = \"\".join([output[0].prompt, output[0].outputs[0].text, \"\\n<|end_of_text|>\"])\n",
    "corrector_flag = \"<|start_of_role|>correction<|end_of_role|>\"\n",
    "corrector_prompt = \"\\n\".join([corrector_input, corrector_flag])\n",
    "output = model_lora.generate(corrector_prompt, sampling_params_lora, lora_request=lora_request)\n",
    "output_text = output[0].outputs[0].text.strip()\n",
    "print(f\"\\ncorrected_text =  {output_text}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddb9e44-1a37-4449-8994-9d363ff7d306",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
